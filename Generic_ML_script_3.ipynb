{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic ML Script 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propscore import PropensityScore\n",
    "import random\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from causalinference import CausalModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the bottom example code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_stata(\"~/OneDrive - London School of Economics/LSE/Year 3/EC331/Afghan/afghanistan_anonymized_data.dta\")\n",
    "\n",
    "treatment = \"treatment\"\n",
    "outcome = \"f07_formal_school\" # enrollment in fall 2007 \n",
    "\n",
    "controls = [\"f07_heads_child_cnt\", \"f07_girl_cnt\", \"f07_age_cnt\", \n",
    "            \"f07_duration_village_cnt\", \"f07_age_head_cnt\", \"f07_yrs_ed_head_cnt\", \n",
    "           \"f07_jeribs_cnt\", \"f07_num_sheep_cnt\", \n",
    "           \"f07_farsi_cnt\", \"f07_tajik_cnt\", \"f07_farmer_cnt\", \"f07_num_ppl_hh_cnt\", \n",
    "           \"f07_nearest_scl\"]            \n",
    "\n",
    "fixed_effects = \"clustercode\" # = None otherwise\n",
    "\n",
    "df['clustercode'] = df['clustercode'].astype('category')\n",
    "states = create_states(df, fixed_effects)\n",
    "\n",
    "cols_to_add = []\n",
    "cols_to_add.append(treatment)\n",
    "cols_to_add.append(outcome)\n",
    "cols_to_add.append(fixed_effects)\n",
    "cols_to_add.extend(controls)\n",
    "\n",
    "df2 = df[cols_to_add]; \n",
    "# df2.join(ps.propscore)\n",
    "df2.loc[:,\"propscore\"] = 5/11\n",
    "df2 = df2.join(states)\n",
    "\n",
    "df2.isnull().sum()\n",
    "df2.dropna(inplace = True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main, aux = sklearn.model_selection.train_test_split(df2, train_size = 0.5)\n",
    "main2 = ML_estimator(main, aux, \"SVM\", treatment, outcome, controls, fixed_effects) \n",
    "\n",
    "BLP(main2, treatment, outcome, 0.05, \"clustercode\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GATES(main2, 5, treatment, outcome, 0.05, \"clustercode\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GATES(main2, 5, treatment, outcome, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to modify BLP(along with the other functions that use standard errors) to use clustered standard errors instead. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Generic_ML_single(df2, treatment, outcome, controls, 10, \"random_forest\", 0.05, 5, fixed_effects = None, robust = \"HC0\") "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ML_models = [\"random_forest\", \"SVM\", \"gradient_boost\", \"neural_net\", \"ElasticNet\"]\n",
    "\n",
    "for x in ML_models: \n",
    "    summary = Generic_ML_single(df2, treatment, outcome, controls, 10, x, 0.05, 5, fixed_effects) \n",
    "    print (str(x) + \": Lamda1: \" + str(summary[-2])+ \" Lambda2: \" + str(summary[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n",
    "To run Generic ML single, we need the dataset to include the propensity scores and fixed effects (if any). \n",
    "\n",
    "Propensity scores \n",
    "1. Calculated from the package propscore or otherwise given by experiment design (e.g. if 50% of individuals are randomly treated, propensity score is 0.5)\n",
    "2. Column for propensity score should be labelled as \"propscore\"\n",
    "\n",
    "Fixed effects\n",
    "1. Dummy variables for all states except for the reference state (to avoid multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generic_ML_single(df, treatment, outcome, controls, iterations = 10, model = \"random_forest\", alpha = 0.05, k = 5, cluster = None, robust = None): \n",
    "    '''\n",
    "    Runs the whole generic ML algorithm for a single ML model and returns a list of datasets for all parameters. Returned objects are:\n",
    "        BLP dataset \n",
    "        GATES dataset \n",
    "        CLAN dataset \n",
    "        Lambda1 - value to choose the best ML estimator (a lower value is better)\n",
    "        Lambda2 - value to choose the best ML estimator (a lower value is better)\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df -- dataframe which must contain the following items: \n",
    "        treatment \n",
    "        outcome\n",
    "        controls\n",
    "        propensity score \n",
    "        B - proxy predictor for BCA \n",
    "        S - proxy predictor for CATE\n",
    "        fixed effects / clusters (if any)\n",
    "    \n",
    "    treatment(string) \n",
    "    outcome(string)\n",
    "    controls(list of strings) \n",
    "    \n",
    "    iterations(numeric) \n",
    "    \n",
    "    model - ML model. Supported models are \"randomforest\", \"SVM\", \"ElasticNet\", \"neuralnet\", \"gradient_boost\"\n",
    "        To add a new model, edit ML_estimator \n",
    "    \n",
    "    alpha -- significance level \n",
    "    \n",
    "    k - number of groups \n",
    "    \n",
    "    fixed effects / cluster -- variable in dataframe indicating the clusters \n",
    "        Clustered standard errors are used if there are clusters \n",
    "    \n",
    "    robust -- robust standard errors. Choose either \"HC0\", \"HC1\", \"HC2\", or \"HC3\"\n",
    "        Cannot be used in conjunction with clustered standard errors. \n",
    "        See statsmodels.regression.linear_model.RegressionResults.get_robustcov_results\n",
    "\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_HET_loop = []\n",
    "    data_ATE_loop = []\n",
    "    lambda1_loop = []\n",
    "\n",
    "    data_GATES_loop = []\n",
    "    data_GATES_loop_extended = []\n",
    "    lambda2_loop = []\n",
    "\n",
    "    data_CLAN_loop = []\n",
    "\n",
    "\n",
    "    for x in range(iterations): \n",
    "        main, aux = sklearn.model_selection.train_test_split(df, train_size = 0.5, random_state = x)\n",
    "        main2 = ML_estimator(main, aux, model, treatment, outcome, controls, cluster) \n",
    "        \n",
    "        # BLP\n",
    "        res_BLP, lambda1 = BLP(main2, treatment, outcome, alpha, cluster, robust)\n",
    "        data_HET, data_ATE = BLP_to_storage(res_BLP)\n",
    "        data_HET_loop.append(data_HET)\n",
    "        data_ATE_loop.append(data_ATE)\n",
    "        lambda1_loop.append(lambda1)\n",
    "    \n",
    "        #GATES\n",
    "        res_GATES, t_test_GATES, lambda2 = GATES(main2, k, treatment, outcome, alpha, cluster, robust) \n",
    "        data_GATES = GATES_to_storage(res_GATES, t_test_GATES, alpha)\n",
    "        data_GATES_loop.append(data_GATES)\n",
    "        lambda2_loop.append(lambda2)\n",
    "        \n",
    "        #GATES extended\n",
    "        data_GATES_extended = GATES_to_storage_extended(res_GATES, t_test_GATES, alpha, k)\n",
    "        data_GATES_loop_extended.append(data_GATES_extended)\n",
    "    \n",
    "        # CLAN\n",
    "        data_CLAN = CLAN(main2, treatment, controls, alpha) # it does not actually make sense to cluster \n",
    "        data_CLAN_loop.append(data_CLAN)\n",
    "\n",
    "        # BLP\n",
    "        data_HET_array = np.array(data_HET_loop)\n",
    "        data_HET_final = np.median(data_HET_array, axis = 0)\n",
    "        data_HET_final[2] = np.minimum(1, data_HET_final[2] *2)\n",
    "\n",
    "        data_ATE_array = np.array(data_ATE_loop)\n",
    "        data_ATE_final = np.median(data_ATE_array, axis = 0)\n",
    "        data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2)   \n",
    "        \n",
    "    \n",
    "    df_BLP = data_BLP_to_df(data_HET_loop, data_ATE_loop)\n",
    "    df_GATES = data_GATES_to_df(data_GATES_loop, k)\n",
    "    df_CLAN = data_CLAN_to_df(data_CLAN_loop, controls = controls)\n",
    "    \n",
    "    lambda1 = np.mean(lambda1_loop)\n",
    "    lamda2 = np.mean(lambda2_loop)\n",
    "    \n",
    "    df_GATES = update_GATES_df(df_GATES, alpha).round(3)\n",
    "    df_CLAN = update_CLAN_df(df_CLAN, controls, alpha).round(3)\n",
    "    \n",
    "    # GATES_extended\n",
    "    df_GATES_extended = data_GATES_to_df_extended(data_GATES_loop_extended, k)\n",
    "    \n",
    "    summary = [df_BLP, df_GATES, df_CLAN, lambda1, lambda2, df_GATES_extended]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP(df, treatment, outcome, alpha, cluster = None, robust = None): \n",
    "    \n",
    "    '''\n",
    "    Finds the Best Linear Predictor (BLP) of the Average Treatment Effect(ATE).\n",
    "    \n",
    "    Returns dataframe of summary results, whose parameters can be used to obtain BLP of Conditional ATE(CATE).\n",
    "    Contains: \n",
    "        Estimator Coefficients of Term 2 and 3 \n",
    "        Standard Error \n",
    "        p values \n",
    "        Confidence Interval (lower and upper bounds)\n",
    "    \n",
    "    Returns lambda1 - value to help choose the best ML method \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df -- (main) dataframe which must contain the following items: \n",
    "        propensity score \n",
    "        B - proxy predictor for BCA \n",
    "        S - proxy predictor for CATE\n",
    "        treatment \n",
    "    \n",
    "    outcome \n",
    "    \n",
    "    alpha -- significance level \n",
    "    \n",
    "    cluster -- variable in dataframe indicating the clusters \n",
    "        Use clustered standard errors if there are clusters \n",
    "    \n",
    "    robust -- robust standard errors. Choose either \"HC0\", \"HC1\", \"HC2\", or \"HC3\"\n",
    "        See statsmodels.regression.linear_model.RegressionResults.get_robustcov_results\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    term2 = df[treatment] - df['propscore']\n",
    "    S = df['S']\n",
    "    term3 = term2 * (S - np.mean(S))\n",
    "    \n",
    "    \n",
    "    combined = df.copy()\n",
    "    combined.loc[:,'term2'] = term2 \n",
    "    combined.loc[:,'term3'] = term3\n",
    "    combined.loc[:,'ones'] = 1 \n",
    "    \n",
    "    X_reg = combined[['B', 'S', 'ones', 'term2', 'term3']]\n",
    "    y = combined[outcome]\n",
    "    \n",
    "    regBLP = sm.OLS(y, X_reg)\n",
    "        \n",
    "    if cluster == None:\n",
    "        if robust == None: \n",
    "           res_BLP = regBLP.fit()\n",
    "        else:\n",
    "            res_BLP = regBLP.fit(cov_type = robust)\n",
    "        \n",
    "    else: \n",
    "        res_BLP = regBLP.fit(cov_type = 'cluster', cov_kwds={'groups': df[[cluster]]})\n",
    "    \n",
    "    res_BLP = results_summary_to_dataframe(res_BLP, alpha)\n",
    "    \n",
    "    lambda1 = res_BLP.iloc[-1,0] * stats.variance(S)   \n",
    "    return res_BLP, lambda1\n",
    "    \n",
    "def results_summary_to_dataframe(results, alpha):\n",
    "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
    "    pvals = results.pvalues\n",
    "    coeff = results.params\n",
    "    std_err = results.bse\n",
    "    \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "\n",
    "    lb = coeff - std_err * crit_val\n",
    "    ub = coeff + std_err * crit_val\n",
    "    \n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               \"lb\":lb,\n",
    "                               \"ub\":ub,\n",
    "                               \"std_err\":std_err, \n",
    "                                })\n",
    "\n",
    "    #Reordering...\n",
    "    results_df = results_df[[\"coeff\",\"std_err\",\"pvals\",\"lb\",\"ub\"]]\n",
    "    return results_df\n",
    "\n",
    "def BLP_to_storage(res_BLP):\n",
    "    \n",
    "    '''\n",
    "    Takes the output of BLP and store them as lists\n",
    "        res_BLP - summary table containing parameters to construct BLP, along with their p-values, standard errors and lower and upper bounds\n",
    "        \n",
    "    Returns 2 lists data_HET and data_ATE whose array-equivalent is of dimension (1 variable, 5 attributes)\n",
    "    '''\n",
    "    \n",
    "    # HET parameter \n",
    "    HET = res_BLP.iloc[-1,0]\n",
    "    HET_se = res_BLP.iloc[-1,1]\n",
    "    HET_pvals = res_BLP.iloc[-1, 2]\n",
    "    HET_lb = res_BLP.iloc[-1, 3]\n",
    "    HET_ub = res_BLP.iloc[-1, 4]\n",
    "\n",
    "    # ATE \n",
    "    ATE = res_BLP.iloc[-2,0]\n",
    "    ATE_se = res_BLP.iloc[-2,1]\n",
    "    ATE_pvals = res_BLP.iloc[-2,2]\n",
    "    ATE_lb = res_BLP.iloc[-2,3]\n",
    "    ATE_ub = res_BLP.iloc[-2,4]\n",
    "    \n",
    "    # Storage\n",
    "    \n",
    "    data_HET = [HET, HET_se, HET_pvals, HET_lb, HET_ub]\n",
    "    data_ATE = [ATE, ATE_se, ATE_pvals, ATE_lb, ATE_ub]\n",
    "\n",
    "    return data_HET, data_ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GATES(df, k, treatment, outcome,  alpha, cluster = None, robust = None): \n",
    "    '''\n",
    "    Returns summary statistics, whose results can give us the average treatment effect \n",
    "    for most and least affected group, and the difference between them. \n",
    "    \n",
    "    Contains: \n",
    "        Estimator Coefficients  \n",
    "        Standard Error \n",
    "        p values \n",
    "        Confidence Interval (lower and upper bounds)\n",
    "        \n",
    "    Returns lambda2 - value to help choose the best ML method \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df -- (main) dataframe which must contain the following items: \n",
    "        propensity score \n",
    "        B - proxy predictor for BCA \n",
    "        S - proxy predictor for CATE\n",
    "        treatment \n",
    "        \n",
    "    k -- number of groups \n",
    "    treatment\n",
    "    outcome \n",
    "    alpha -- significance level \n",
    "    cluster\n",
    "    robust \n",
    "    '''\n",
    "    \n",
    "    combined = df.copy()\n",
    "    term2 = df[treatment] - df['propscore']\n",
    "    combined.loc[:,'term2'] = term2\n",
    "    combined.loc[:,'ones'] = 1\n",
    "    \n",
    "    groups = groups_multiply(df, group_create(k, df), treatment, k)\n",
    "    combined = pd.concat([combined,groups], axis = 1) \n",
    "  \n",
    "    controls_GATES = [\"B\", \"S\", \"ones\"] + [\"G\" + str(i) for i in range(1,k+1)]\n",
    "    X_GATES = combined[controls_GATES] # modify for auto selection of columns\n",
    "    y = combined[outcome]\n",
    "    \n",
    "    regGATES = sm.OLS(y, X_GATES)\n",
    "    if cluster == None: \n",
    "        if robust == None: \n",
    "            res_GATES = regGATES.fit()\n",
    "        else: \n",
    "            res_GATES = regGATES.fit(cov_type = robust)\n",
    "    \n",
    "    else: \n",
    "        res_GATES = regGATES.fit(cov_type = \"cluster\", cov_kwds={'groups': df[[cluster]]})\n",
    "    \n",
    "    # Hypothesis testing \n",
    "    hypothesis = \"(G1 = \" + \"G\" + str(k) + \")\" # G1 = G{k}\n",
    "    t_test_html = res_GATES.t_test(hypothesis).summary().as_html()\n",
    "    t_test = pd.read_html(t_test_html, header=0, index_col=0)[0] # is there some sort of clustered standard errors here\n",
    "    \n",
    "    res_GATES = results_summary_to_dataframe(res_GATES, alpha)\n",
    "    \n",
    "    lambda2 = res_GATES.iloc[3:, 0].mean()**2 / k\n",
    "    \n",
    "    return res_GATES, t_test, lambda2\n",
    "    \n",
    "\n",
    "def group_create(k, df): \n",
    "    '''\n",
    "    Returns quantiles of the variable 'S', encoded into dummy variables\n",
    "    '''\n",
    "    breaks = df['S'].quantile(np.linspace(0,1,(k+1)))\n",
    "    breaks.iloc[0,] = breaks.iloc[0,] - 0.001 \n",
    "    breaks.iloc[k,] = breaks.iloc[k,] - 0.001 \n",
    "    \n",
    "    combined = df.copy()\n",
    "    combined['Groups'] = pd.cut(x= df['S'], bins = breaks) # this will fail if there are too many groups\n",
    "    groups = pd.get_dummies(combined['Groups'])\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def groups_multiply(df, groups, treatment, k):\n",
    "    '''\n",
    "    Multiply dataframe generated by function group_create with term 2 and rename columns \n",
    "    '''\n",
    "    \n",
    "    combined = df.copy()\n",
    "    term2 = df[treatment] - df['propscore']\n",
    "    combined.loc[:,'term2'] = term2\n",
    "    \n",
    "    groups = np.multiply(groups, combined['term2'].values.reshape(len(df.index),1))\n",
    "    groups.columns = [\"G\" + str(i) for i in range(1,k+1)] \n",
    "    \n",
    "    return groups\n",
    "\n",
    "def GATES_to_storage(res_GATES, t_test_GATES, alpha):\n",
    "    \n",
    "    '''\n",
    "    Takes the output of GATES and store them as lists, whereby the output refers to: \n",
    "        res_GATES - summary table containing parameters to construct GATES, along with their p-values and standard errors \n",
    "        t_test_GATEs - t test table to determine if G1 = Gk \n",
    "    \n",
    "    Returns a list whose array-equivalent is dimension of (# of variables, # of attributes )\n",
    "    '''\n",
    "    \n",
    "    # Most affected group \n",
    "    gamma1 = res_GATES.iloc[3,0]\n",
    "    gamma1_se = res_GATES.iloc[3,1]\n",
    "    gamma1_pvals = res_GATES.iloc[3,2]\n",
    "    gamma1_lb = res_GATES.iloc[3,3]\n",
    "    gamma1_ub = res_GATES.iloc[3,4]\n",
    "\n",
    "    # Least affected group \n",
    "    gammak = res_GATES.iloc[-1,0]\n",
    "    gammak_se = res_GATES.iloc[-1,1]\n",
    "    gammak_pvals = res_GATES.iloc[-1,2]\n",
    "    gammak_lb = res_GATES.iloc[-1,3]\n",
    "    gammak_ub = res_GATES.iloc[-1,4]\n",
    "    \n",
    "    # Difference between most and least affected group \n",
    "  \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "\n",
    "    gamma_diff = abs(t_test_GATES.iloc[0,0])\n",
    "    gamma_diff_se = t_test_GATES.iloc[0,1]\n",
    "    gamma_diff_pvals = t_test_GATES.iloc[0,3] \n",
    "    gamma_diff_lb = gamma_diff - crit_val * gamma_diff_se\n",
    "    gamma_diff_ub = gamma_diff + crit_val * gamma_diff_se\n",
    "    \n",
    "    data_gamma1 = [gamma1, gamma1_se, gamma1_pvals, gamma1_lb, gamma1_ub]\n",
    "    data_gammak = [gammak, gammak_se, gammak_pvals, gammak_lb, gammak_ub]\n",
    "    data_gamma_diff = [gamma_diff, gamma_diff_se, gamma_diff_pvals, gamma_diff_lb, gamma_diff_ub]\n",
    "\n",
    "    data_gamma = [data_gamma1, data_gammak, data_gamma_diff]\n",
    "    \n",
    "    return data_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLAN(df, treatment, controls, alpha, k = 5, robust = None):\n",
    "    \n",
    "    '''\n",
    "    Finds the average of the controls for the most and least affected groups and the difference between them.  \n",
    "    \n",
    "    Returns this as a list. \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df -- (main) dataframe which must contain the following items: \n",
    "        propensity score \n",
    "        B - proxy predictor for BCA \n",
    "        S - proxy predictor for CATE\n",
    "        treatment \n",
    "        controls \n",
    "    \n",
    "    treatment (string) \n",
    "    controls (list of column names)\n",
    "    alpha -- significance level \n",
    "    k -- number of groups\n",
    "\n",
    "    '''\n",
    "    \n",
    "    data_CLAN_loop = []\n",
    "    for x in controls: \n",
    "        res_CLAN, t_test = CLAN_single(df, treatment, x, alpha, k, robust)\n",
    "        data_CLAN = CLAN_to_storage(res_CLAN, t_test, alpha)\n",
    "        data_CLAN_loop.append(data_CLAN)\n",
    "    return data_CLAN_loop\n",
    "\n",
    "def CLAN_single(df, treatment, control, alpha, k, robust):\n",
    "    \n",
    "    '''\n",
    "    Gives the average characteristic for a control for the most and least affected group and the difference between these groups. \n",
    "        Uses group membership as defined in GATES and runs a regression of a control on dummy variables for group membership. \n",
    "        The coefficients are the effect of group membership on the control variable or the mean of the control for that particular group. \n",
    "        Difference between most and least affected is derived from a t test. \n",
    "    \n",
    "    Returns 2 dataframes. \n",
    "        The first dataframe is for the most and least affected group. \n",
    "        The second dataframe is the difference between them. \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df -- (main) dataframe which must contain the following items: \n",
    "        propensity score \n",
    "        B - proxy predictor for BCA \n",
    "        S - proxy predictor for CATE\n",
    "        treatment \n",
    "        control \n",
    "        \n",
    "    treatment\n",
    "    control \n",
    "    k -- number of groups\n",
    "    '''\n",
    "    \n",
    "    combined = df.copy()\n",
    "    term2 = combined[treatment] - combined['propscore']\n",
    "    combined.loc[:,'term2'] = term2\n",
    "    combined.loc[:,'ones'] = 1\n",
    "\n",
    "    groups_df = group_create(k, combined)\n",
    "    groups_df.columns = [\"G\" + str(i) for i in range(1,k+1)]\n",
    "    combined = pd.concat([combined , groups_df], axis = 1) \n",
    "\n",
    "    X_control = combined[[\"G\" + str(i) for i in range(1,k+1)]] \n",
    "    y_control = combined[[control]]\n",
    "    \n",
    "    reg_CLAN = sm.OLS(y_control, X_control)\n",
    "    \n",
    "    if robust == None: \n",
    "        res_CLAN = reg_CLAN.fit()\n",
    "    else: \n",
    "        res_CLAN = reg_CLAN.fit(cov_type = robust)\n",
    "        \n",
    "    hypothesis = \"(G1 = \" + \"G\" + str(k) + \")\" # G1 = G{k}\n",
    "    t_test_html = res_CLAN.t_test(hypothesis).summary().as_html() # do I need to use clustered standard errors here\n",
    "    t_test = pd.read_html(t_test_html, header=0, index_col=0)[0]\n",
    "\n",
    "    res_CLAN = results_summary_to_dataframe(res_CLAN, alpha)\n",
    "    \n",
    "    return res_CLAN, t_test\n",
    "\n",
    "def CLAN_to_storage(res_CLAN, t_test, alpha):\n",
    "    \n",
    "    '''\n",
    "    Takes the summary results of CLAN_single and store them as lists. \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    res_CLAN -- first dataframe returned from CLAN_single \n",
    "    t_test -- second dataframe returned from CLAN_single\n",
    "    alpha -- significance level \n",
    "    '''\n",
    "    \n",
    "    h_coeff = res_CLAN.iloc[0,0]\n",
    "    h_se = res_CLAN.iloc[0,1]\n",
    "    h_pvals = res_CLAN.iloc[0,2]\n",
    "    h_lb = res_CLAN.iloc[0,3]\n",
    "    h_ub = res_CLAN.iloc[0,4]\n",
    "    data_h = [h_coeff, h_se, h_pvals, h_lb, h_ub]\n",
    "    \n",
    "    l_coeff = res_CLAN.iloc[-1,0]\n",
    "    l_se = res_CLAN.iloc[-1,1]\n",
    "    l_pvals = res_CLAN.iloc[-1,2]\n",
    "    l_lb = res_CLAN.iloc[-1,3]\n",
    "    l_ub = res_CLAN.iloc[-1,4]\n",
    "    data_l = [l_coeff, l_se, l_pvals, l_lb, l_ub]\n",
    "    \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "    \n",
    "    diff_coeff = t_test.iloc[0,0]\n",
    "    diff_se = t_test.iloc[0,1]\n",
    "    diff_pvals = t_test.iloc[0,3]\n",
    "    diff_lb = diff_coeff - crit_val * diff_se\n",
    "    diff_ub = diff_coeff + crit_val * diff_se\n",
    "    data_diff = [diff_coeff, diff_se, diff_pvals, diff_lb, diff_ub]\n",
    "    \n",
    "    data_CLAN = data_h, data_l, data_diff\n",
    "    return data_CLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to dataframes and Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_BLP_to_df(data_HET_loop, data_ATE_loop): \n",
    "    '''\n",
    "    Takes the data of BLP stored as a list, find its median over different iterations and adjusts p values. \n",
    "    \n",
    "    Returns it as a dataframe \n",
    "    '''\n",
    "    \n",
    "    data_HET_array = np.array(data_HET_loop)\n",
    "    data_HET_final = np.median(data_HET_array, axis = 0)\n",
    "    data_HET_final[2] = np.minimum(1, data_HET_final[2] *2)\n",
    "\n",
    "    data_ATE_array = np.array(data_ATE_loop)\n",
    "    data_ATE_final = np.median(data_ATE_array, axis = 0)\n",
    "    data_ATE_final[2] = np.minimum(1, data_ATE_final[2] * 2)   \n",
    "    \n",
    "    df_ATE = pd.DataFrame(data_ATE_final, \n",
    "                     index = ['coeff', 'se', 'pvalue', 'lower bound', 'upper bound'], \n",
    "                     columns = ['ATE'])\n",
    "\n",
    "    df_HET = pd.DataFrame(data_HET_final, \n",
    "                     index = ['coeff', 'se', 'pvalue', 'lower bound', 'upper bound'], \n",
    "                     columns = ['HET'])\n",
    "\n",
    "    frames = [df_ATE, df_HET]\n",
    "    \n",
    "    df_BLP = pd.concat(frames, axis = 1)\n",
    "    \n",
    "    return df_BLP\n",
    "    \n",
    "def data_GATES_to_df(data_GATES_loop, groups): \n",
    "    '''\n",
    "    Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. \n",
    "    \n",
    "    Returns it as a dataframe \n",
    "    '''\n",
    "    \n",
    "    # GATES \n",
    "    data_GATES_array = np.array(data_GATES_loop)\n",
    "    data_GATES_final = np.median(data_GATES_array, axis = 0)\n",
    "    data_GATES_final[:, 2] = np.minimum(1, data_GATES_final[:, 2]* 2)\n",
    "    \n",
    "    df_GATES = pd.DataFrame(data_GATES_final, \n",
    "                        columns = ['coeff', 'se', 'pvalue', 'lower bound', 'upper bound'], \n",
    "                        index = ['least affected(' + str(100/groups) + \"%)\", 'most affected(' + str(100 - 100/groups) + \"%)\", 'most - least affected'])\n",
    "    \n",
    "    return df_GATES.transpose()\n",
    "\n",
    "def data_CLAN_to_df(data_CLAN_loop, controls): \n",
    "    '''\n",
    "    Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. \n",
    "    \n",
    "    Returns it as a dataframe \n",
    "    '''\n",
    "    \n",
    "    # CLAN \n",
    "    data_CLAN_array = np.array(data_CLAN_loop) \n",
    "\n",
    "\n",
    "    data_CLAN_final = np.median(data_CLAN_array, axis = 0) # This code is technically wrong as we take the upper medians for the lower bounds\n",
    "    data_CLAN_final[0,2,:] = np.minimum(1, data_CLAN_final[0,2,:] * 2)\n",
    "    \n",
    "    list = []\n",
    "    for x in controls: \n",
    "        list1 = ['Least affected (' + str(x) + \")\", 'Most affected (' + str(x) + \")\", 'Most - Least affected (' + str(x) + \")\" ]\n",
    "        list.append(list1)\n",
    "    \n",
    "    flattened_list = [y for x in list for y in x]\n",
    "\n",
    "    data_CLAN_new = data_CLAN_final.reshape(-1,5)\n",
    "    df_CLAN = pd.DataFrame(data_CLAN_new, \n",
    "                      columns = ['coeff', 'se', 'pvalue', 'lower bound', 'upper bound'], \n",
    "                      index = flattened_list)\n",
    "\n",
    "    return df_CLAN\n",
    "\n",
    "def update_GATES_df(df, alpha):\n",
    "    \n",
    "    '''\n",
    "    Take the GATES dataframe and ensures that the difference between the most and least affected group \n",
    "    is the ATE of most affected - ATE of least affected\n",
    "    '''\n",
    "    \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "    \n",
    "    df.iloc[0,-1] = df.iloc[0,1] - df.iloc[0,0]\n",
    "    df.iloc[3,-1] = df.iloc[0,-1] -  df.iloc[1,-1] *  crit_val\n",
    "    df.iloc[4,-1] = df.iloc[0,-1] +  df.iloc[1,-1] *  crit_val\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_CLAN_df(df, controls, alpha):\n",
    "    \n",
    "    '''\n",
    "    Take the CLAN dataframe and ensures that the difference between the most and least affected group \n",
    "    is the ATE of most affected - ATE of least affected\n",
    "    '''\n",
    "    \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "    # Most - least \n",
    "    \n",
    "    for i, x in enumerate(controls, start = 1): \n",
    "        y = 3*(i)\n",
    "        df.iloc[(y-1) , 0] = df.iloc[(y -2),0] - df.iloc[(y -3),0]\n",
    "        df.iloc[(y-1),3] = df.iloc[(y -1),0] -  df.iloc[(y -1),1] *  crit_val\n",
    "        df.iloc[(y-1),4] = df.iloc[(y -1),0] +  df.iloc[(y -1),1] *  crit_val\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_states(df, fixed_effects):\n",
    "    '''\n",
    "    Create binary variables for the fixed effects and drop one of them\n",
    "    '''\n",
    "    states = pd.get_dummies(df[fixed_effects])\n",
    "    states.drop(states.columns[0], axis = 1, inplace = True)\n",
    "    \n",
    "    return states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_estimator(main, aux, model, treatment, outcome, controls, fixed_effects = None):\n",
    "    '''\n",
    "    Returns the main dataset combined with B and S, which are proxy predictors for BCA and CATE respectively \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    main: main dataset which must contain treatment and outcome\n",
    "    aux: auxilliary dataset which must contain treatment and outcome\n",
    "    model - in string format \n",
    "        supported models = [\"random_forest\", \"SVM\", \"gradient_boost\", \"neural_net\", \"ElasticNet\"]\n",
    "    treatment - in str\n",
    "    outcome - in str \n",
    "    com\n",
    "    \n",
    "    # need to set the seed of the ML_estimators\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialization\n",
    "    if fixed_effects == None: \n",
    "        aux0 = aux[aux[treatment] == 0]\n",
    "        aux1 = aux[aux[treatment] == 1]\n",
    "        X_aux0 = aux0.loc[:,[treatment] + controls]\n",
    "        y_aux0 =aux0[outcome]\n",
    "        X_aux1 = aux1.loc[:,[treatment] + controls]\n",
    "        y_aux1 =aux1[outcome]\n",
    "    \n",
    "        X_main = main.loc[:,[treatment] + controls]\n",
    "        y_main = main[outcome]\n",
    "        \n",
    "    else: \n",
    "        states = create_states(main, fixed_effects) # need to have same column names as in the original dataframe\n",
    "        \n",
    "        cols = [treatment] + controls + list(states.columns)\n",
    "        aux0 = aux[aux[treatment] == 0]\n",
    "        aux1 = aux[aux[treatment] == 1]\n",
    "        X_aux0 = aux0.loc[:,cols]\n",
    "        y_aux0 =aux0[outcome]\n",
    "        X_aux1 = aux1.loc[:,cols]\n",
    "        y_aux1 =aux1[outcome]\n",
    "    \n",
    "        X_main = main.loc[:,cols]\n",
    "        y_main = main[outcome]\n",
    "    \n",
    "    \n",
    "    # Model \n",
    "    if model == \"random_forest\": \n",
    "        combined = random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1)\n",
    "    elif model == \"SVM\": \n",
    "        combined = SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1)\n",
    "    elif model == \"gradient_boost\": \n",
    "        combined = gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1)\n",
    "    elif model == \"neural_net\": \n",
    "        combined = neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1)\n",
    "    elif model == \"ElasticNet\": \n",
    "        combined = ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1)\n",
    "    \n",
    "    # Add variance\n",
    "    if stats.variance(combined['S']) == 0 : \n",
    "        combined['S'] = combined['S'] + np.random.normal(0,0.1, len(combined['S'])) \n",
    "    if stats.variance(combined['B']) == 0 : \n",
    "        combined['B'] = combined['B'] + np.random.normal(0,0.1, len(combined['B'])) \n",
    "        \n",
    "    return combined\n",
    "\n",
    "def random_forest(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1):\n",
    "    \n",
    "    # Model \n",
    "    clf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    \n",
    "    clf.fit(X_aux0, y_aux0)\n",
    "    B = clf.predict(X_main)\n",
    "\n",
    "    clf.fit(X_aux1, y_aux1)\n",
    "    clf.predict(X_main)\n",
    "    S = clf.predict(X_main) - B \n",
    "    \n",
    "    combined = main.copy()\n",
    "    combined['B'] = B \n",
    "    combined['S'] = S\n",
    "        \n",
    "    return combined\n",
    "\n",
    "def SVM(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1):\n",
    "\n",
    "    # Model \n",
    "    clf = svm.SVR()\n",
    "    \n",
    "    clf.fit(X_aux0, y_aux0)\n",
    "    B = clf.predict(X_main)\n",
    "\n",
    "    clf.fit(X_aux1, y_aux1)\n",
    "    clf.predict(X_main)\n",
    "    S = clf.predict(X_main) - B \n",
    "    \n",
    "    combined = main.copy()\n",
    "    combined['B'] = B \n",
    "    combined['S'] = S\n",
    "        \n",
    "    return combined\n",
    "\n",
    "def gradient_boost(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1):\n",
    "\n",
    "    \n",
    "    params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.01,\n",
    "          'loss': 'ls'}\n",
    "    \n",
    "    # Model \n",
    "    clf = ensemble.GradientBoostingRegressor(**params)\n",
    "    \n",
    "    clf.fit(X_aux0, y_aux0)\n",
    "    B = clf.predict(X_main)\n",
    "\n",
    "    clf.fit(X_aux1, y_aux1)\n",
    "    clf.predict(X_main)\n",
    "    S = clf.predict(X_main) - B \n",
    "    \n",
    "    combined = main.copy()\n",
    "    combined['B'] = B \n",
    "    combined['S'] = S\n",
    "        \n",
    "    return combined\n",
    "\n",
    "def neural_net(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1):\n",
    "    \n",
    "    # Model \n",
    "    clf = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    \n",
    "    clf.fit(X_aux0, y_aux0)\n",
    "    B = clf.predict(X_main)\n",
    "\n",
    "    clf.fit(X_aux1, y_aux1)\n",
    "    clf.predict(X_main)\n",
    "    S = clf.predict(X_main) - B \n",
    "    \n",
    "    combined = main.copy()\n",
    "    combined['B'] = B \n",
    "    combined['S'] = S\n",
    "        \n",
    "    return combined\n",
    "\n",
    "def ElasticNet(main, X_aux0, y_aux0, X_main, X_aux1, y_aux1):\n",
    "        \n",
    "    # Model \n",
    "    clf = sklearn.linear_model.ElasticNet()\n",
    "    \n",
    "    clf.fit(X_aux0, y_aux0)\n",
    "    B = clf.predict(X_main)\n",
    "\n",
    "    clf.fit(X_aux1, y_aux1)\n",
    "    clf.predict(X_main)\n",
    "    S = clf.predict(X_main) - B \n",
    "    \n",
    "    combined = main.copy()\n",
    "    combined['B'] = B \n",
    "    combined['S'] = S\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATES extended\n",
    "\n",
    "Here, we do GATES but keep the information for all subgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_GATES_to_df_extended(data_GATES_loop_extended, groups): \n",
    "    '''\n",
    "    Takes the data of GATES stored as a list, find its median over different iterations and adjusts p values. \n",
    "    \n",
    "    Returns it as a dataframe \n",
    "    '''\n",
    "    \n",
    "    # GATES extended\n",
    "    data_GATES_array = np.array(data_GATES_loop_extended)\n",
    "    data_GATES_final = np.median(data_GATES_array, axis = 0)\n",
    "    data_GATES_final[:, 2] = np.minimum(1, data_GATES_final[:, 2]* 2) # p values adjustment\n",
    "    \n",
    "    # Index \n",
    "    index = []\n",
    "    for i in range(1, 1 + groups):\n",
    "        list = [\"G\" + str(i)]\n",
    "        index.append(list)\n",
    "\n",
    "    index = [y for x in index for y in x]\n",
    "    index.append(\"most - least affected\")    \n",
    "    \n",
    "    df_GATES_extended = pd.DataFrame(data_GATES_final, \n",
    "                        columns = ['coeff', 'se', 'pvalue', 'lower bound', 'upper bound'], \n",
    "                        index = index)\n",
    "                       \n",
    "    return df_GATES_extended.transpose()\n",
    "\n",
    "\n",
    "def GATES_to_storage_extended(res_GATES, t_test_GATES, alpha, groups):\n",
    "    \n",
    "    '''\n",
    "    Takes the output of GATES and store them as lists, whereby the output refers to: \n",
    "        res_GATES - summary table containing parameters to construct GATES, along with their p-values and standard errors \n",
    "        t_test_GATEs - t test table to determine if G1 = Gk \n",
    "    \n",
    "    Returns a list whose array-equivalent is dimension of (# of variables, # of attributes )\n",
    "    '''\n",
    "    \n",
    "    data_gamma = []\n",
    "    \n",
    "    # All subgroups \n",
    "    for i in range(1, groups +1):\n",
    "        gammai = res_GATES.iloc[2 + i, 0]\n",
    "        gammai_se = res_GATES.iloc[2 + i,1]\n",
    "        gammai_pvals = res_GATES.iloc[2 + i,2]\n",
    "        gammai_lb = res_GATES.iloc[2 + i,3]\n",
    "        gammai_ub = res_GATES.iloc[2 + i,4]\n",
    "        \n",
    "        data_gammai = [gammai, gammai_se, gammai_pvals, gammai_lb, gammai_ub]\n",
    "        data_gamma.append(data_gammai)\n",
    "    \n",
    "    # Difference between most and least affected group \n",
    "  \n",
    "    crit_val = norm.ppf(1-alpha/2) \n",
    "\n",
    "    gamma_diff = abs(t_test_GATES.iloc[0,0])\n",
    "    gamma_diff_se = t_test_GATES.iloc[0,1]\n",
    "    gamma_diff_pvals = t_test_GATES.iloc[0,3] \n",
    "    gamma_diff_lb = gamma_diff - crit_val * gamma_diff_se\n",
    "    gamma_diff_ub = gamma_diff + crit_val * gamma_diff_se\n",
    "    \n",
    "    data_gamma_diff = [gamma_diff, gamma_diff_se, gamma_diff_pvals, gamma_diff_lb, gamma_diff_ub]\n",
    "    \n",
    "    # Combine everything together\n",
    "    \n",
    "    data_gamma.append(data_gamma_diff)\n",
    "    \n",
    "    return data_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
